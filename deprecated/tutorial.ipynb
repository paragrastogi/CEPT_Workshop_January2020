{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How I learned to stop worrying and love uncertainty </h1>\n",
    "\n",
    "An introductory workshop on quantifying uncertainty in building simulation.\n",
    "\n",
    "<b>author</b>: Parag Rastogi; <b>venue</b>: CEPT University, Ahmedabad; <b>date</b>: 05-06 January, 2019.\n",
    "\n",
    "Run each module one-by-one by either using the <kbd>run cell</kbd> button above or pressing <kbd>Ctrl + Enter</kbd> when a cell is selected. Modules like this one are `Markdown` modules, which is a kind of text-encoding language. These will not produce an output - instead you will see formatted text in the cell when you run one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialisation</h2>\n",
    "\n",
    "<ol>\n",
    "<li>If you haven't read the wiki, I suggest starting <a href='https://github.com/paragrastogi/CEPT_Workshop_January2019/wiki'>there</a>. \n",
    "\n",
    "<li> Download the weather data given <a href='https://drive.google.com/drive/folders/15hyS_Rg-DMxu05FkoS_K-NfFdAozvgJK?usp=sharing'>here</a>. You will see three folders in there: <code>India_Amravati</code>, <code>Switzerland_Geneva</code>, and <code>India_Dehradun</code>. Download Geneva and Dehradun for this example,  placing them in the top-level directory where this script and others are located, i.e., at the same level as <code>lib</code>. For example, on my computer this file is in <code>C:\\GitWorkspace\\CEPT_Workshop_January2019</code>.\n",
    "\n",
    "</ol>\n",
    "\n",
    "Everything should work ok on a Mac, though I have only tested the exercises on Windows and Ubuntu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the most basic things.\n",
    "\n",
    "# This notebook is an instance of python, sometimes called a 'kernel'.\n",
    "# Any variable you declare (define, specify) here will persist until you restart the kernel.\n",
    "\n",
    "# Comments start with a hash sign. Text or numbers that are not preceded by a \n",
    "# hash sign will be interpreted as code.\n",
    "\n",
    "# This cell contains python code, while the cell above contains markdown and/or html code.\n",
    "# Markdown is a language for rendering text.\n",
    "\n",
    "# For example, declare a new variable:\n",
    "new_variable = 2 + 3\n",
    "\n",
    "print(new_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable declared above is still available in this cell.\n",
    "print(new_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few different kinds of objects in python:\n",
    "\n",
    "# 1. A number (integer or float).\n",
    "new_number = 1\n",
    "print(new_number)\n",
    "# You can add two numbers together.\n",
    "print(new_number + new_variable) # New variable is still hanging around.\n",
    "\n",
    "# 2. A string (text).\n",
    "new_string = 'lifafa'\n",
    "print(new_string)\n",
    "# You can add to a string or remove.\n",
    "print(new_string.replace('f', 'ph'))\n",
    "\n",
    "# 3. A list.\n",
    "new_list = ['mangoes', 'watermelon', 'jamun']\n",
    "\n",
    "# You can add to a list.\n",
    "new_list.append('kaku')\n",
    "\n",
    "# 4. A dictionary.\n",
    "new_dict = dict(apples=1, oranges='tasty', bananas='bananas')\n",
    "another_dict = {'apples':2, 'oranges':'bitter', 'bananas':'bananananas'}\n",
    "yet_another_dict = dict(fruits=new_list, numbers=[2,8,4,6])\n",
    "\n",
    "print('new_dict', new_dict)\n",
    "print('another_dict', another_dict)\n",
    "print('yet_another_dict', yet_another_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scientific Programming in Python</h2> \n",
    "\n",
    "Python contains a lot of in-built commands but its functionality can be super-charged with external libraries or modules. The libraries I use frequently are <kbd>pandas</kbd>, <kbd>numpy</kbd>, and <kbd>sklearn</kbd>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Types of uncertainty and exercises</h2>\n",
    "\n",
    "The uncertainty in inputs can be separated into two types:\n",
    "\n",
    "<ol>\n",
    "    <li> Epistemic - lack of knowledge\n",
    "    <li> Aleatory - inherent randomness\n",
    "</ol>\n",
    "\n",
    "In general, epistemic uncertainty can be reduced with better knowledge but aleatory cannot. Sometimes natural phenomena that appear aleatory, as in, inherently random, may not be so; it could just be that our lack of knowledge about these phenomena makes them seem random. The climate is a good example of this.\n",
    "\n",
    "<strong>Exercise: Write down two examples of epistemic and aleatory uncertainty related to building simulation, performance, or design.</strong>\n",
    "\n",
    "============================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a regression model (black-box model) to estimate the response of a building. We will do an exercise related to epistemic uncertainty and one related to aleatory. The rest of the workshop is structured as follows:\n",
    "\n",
    "<ol>\n",
    "    <li> Loading python modules and weather files.\n",
    "    <li> Black-box models.\n",
    "    <li> Epistemic uncertainty with black-box models.\n",
    "    <li> Aleatory uncertainty with black-box models.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various standard modules.\n",
    "# os gives you access to various functions related to the file system. \n",
    "import os\n",
    "# pickle is used to compress and store data. \n",
    "import pickle\n",
    "# There's a good reason this package is used to make copies of variables.\n",
    "import copy\n",
    "\n",
    "# pandas is a great package for manipulating data.\n",
    "import pandas as pd\n",
    "# numpy gives you various maths functions.\n",
    "import numpy as np\n",
    "\n",
    "# We will use a scaler to ensure that all the inputs are of roughly the same order of magnitude. \n",
    "# This does not make much of a difference when you fit regression models like we did above,\n",
    "# since the magnitudes of the coefficients match those of the features to produce the correct\n",
    "# magnitude of output. \n",
    "# However, we need it here to demonstrate epistemic uncertainty.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotting modules.\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Get inline graphs .\n",
    "%matplotlib inline\n",
    "\n",
    "# Only useful for debugging, when you \n",
    "# need to reload external modules.\n",
    "from importlib import reload\n",
    "\n",
    "# Enable xkcd mode - if you're a nerd.\n",
    "# plt.xkcd()\n",
    "\n",
    "# Import a custom read-write function for weather files.\n",
    "import wfileio as wf\n",
    "\n",
    "# import small helpers I've written.\n",
    "from petites import circ_rolling_mean\n",
    "from petites import smoother\n",
    "\n",
    "# Import an awesome colour palette. Call it colours.\n",
    "import default_colours as colours\n",
    "\n",
    "# Set the random seed to a specific value so\n",
    "# the experiment is repeatable.\n",
    "# See  https://en.wikipedia.org/wiki/Random_seed  for more information on what this means.\n",
    "# For why I chose 42, read The Hitchhiker's Guide to the Galaxy.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read location data.\n",
    "\n",
    "# We will store some metadata about the stations as well.\n",
    "metadata = dict(locdata=list(), header=list())\n",
    "\n",
    "# We will only use the EPW file format for this exercise.\n",
    "file_type = 'epw'\n",
    "\n",
    "# Windows uses the backslash '\\' as a separator while Unix/Mac use the forward slash '/'.\n",
    "# It is safe to use the forward slash inside python since it is intelligent enough\n",
    "# to figure out which OS it is running on and change it when required.\n",
    "path_epw_ddn = '../India_Dehradun/IND_UT_Dehradun.421110_ISHRAE2014.epw'\n",
    "\n",
    "# The small program get_weather stores data from the incoming weather\n",
    "# file as a dataframe.\n",
    "ddn, locdata, header = wf.get_weather('ddn', path_epw_ddn)\n",
    "\n",
    "# Save some metadata about the location.\n",
    "metadata['locdata'].append(locdata)\n",
    "metadata['header'].append(header)\n",
    "\n",
    "# Do the same with Geneva data. Except Geneva has multiple weather files, so use a loop.\n",
    "path_gen_folder = '../Switzerland_Geneva/historical'\n",
    "list_genfiles = os.listdir(path_gen_folder) # + '/*.{:s}'.format(file_type))\n",
    "list_genfiles_paths = [path_gen_folder + '/' + f for f in list_genfiles if 'epw' in f]\n",
    "\n",
    "# Declare a list.\n",
    "gen_list = list()\n",
    "\n",
    "for file in list_genfiles_paths:\n",
    "    \n",
    "    gen_temp, locdata, header = wf.get_weather('gen', file)\n",
    "    gen_list.append(gen_temp)\n",
    "    \n",
    "# Keep the list - useful for plotting and stuff later.\n",
    "gen = pd.concat(gen_list)\n",
    "\n",
    "metadata['locdata'].append(locdata)\n",
    "metadata['header'].append(header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEHRADUN')\n",
    "\n",
    "print(ddn.describe())\n",
    "\n",
    "print('======\\r\\n')\n",
    "\n",
    "print('GENEVA')\n",
    "\n",
    "print(gen.describe())\n",
    "\n",
    "print('======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the monthly mean values of weather parameters for Dehradun.\n",
    "\n",
    "relevant_keys = ['tdb']\n",
    "span = 730  # About a month.\n",
    "\n",
    "# Calculate the monthly mean of temperature for plotting.\n",
    "ddn_summary = ddn.resample('1M').mean()[relevant_keys]\n",
    "# Assign an index to the dataframe - useful for plotting.    \n",
    "ddn_summary.index = pd.unique(ddn['month'])\n",
    "\n",
    "# Calculate a smoothed version of the original hourly signal using a rolling mean.\n",
    "ddn_smooth = circ_rolling_mean(ddn[relevant_keys])\n",
    "\n",
    "# Do the same for Geneva.\n",
    "gen_summary = list()\n",
    "gen_smooth = list()\n",
    "for yearlong in gen_list:\n",
    "    \n",
    "    temp_summary = yearlong.resample('1M').mean()[relevant_keys]\n",
    "    # Assign an index to the dataframe - useful for plotting.    \n",
    "    temp_summary.index = pd.unique(yearlong['month'])\n",
    "    gen_summary.append(temp_summary)\n",
    "\n",
    "    # Calculate a smoothed version of the original hourly signal using a rolling mean.\n",
    "    gen_smooth.append(circ_rolling_mean(yearlong[relevant_keys]))\n",
    "\n",
    "# # Set the variable 'tmy' to the incoming data from Dehradun - so changing \n",
    "# # this statement will allow you to run the worksheet with any location.\n",
    "# tmy = ddn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(16, 12)) \n",
    "ax = axes.flatten()\n",
    "fig.tight_layout(pad=3, w_pad=2, h_pad=5)\n",
    "\n",
    "# Find the index and value of the max and min temperatures.\n",
    "ymax = list()\n",
    "ymin = list()\n",
    "xmax = list()\n",
    "xmin = list()\n",
    "\n",
    "ddn_temp_vals = ddn['tdb'].values\n",
    "ymax.append(np.max(ddn_temp_vals))\n",
    "xmax.append(np.argmax(ddn_temp_vals))\n",
    "ymin.append(np.min(ddn_temp_vals))\n",
    "xmin.append(np.argmin(ddn_temp_vals))\n",
    "\n",
    "gen_temp_vals = gen['tdb'].values\n",
    "ymax.append(np.max(gen_temp_vals))\n",
    "ymin.append(np.min(gen_temp_vals))\n",
    "\n",
    "plot_x = np.arange(0, ddn.shape[0])\n",
    "ax[0].plot(plot_x, ddn_smooth['tdb'].values, zorder=10, color=colours.blue)\n",
    "ax[0].plot(plot_x, ddn['tdb'].values, zorder = 1, color=colours.orange, alpha=0.5)\n",
    "ax[0].plot(plot_x, np.repeat(ddn_summary['tdb'], 8760/12), color=colours.blackest, zorder=11)\n",
    "\n",
    "\n",
    "for raw, smooth, summary in zip(gen_list, gen_smooth, gen_summary):\n",
    "    \n",
    "    p1 = ax[1].plot(plot_x, smooth['tdb'], zorder=10, color=colours.blue, alpha=0.75)\n",
    "    \n",
    "    raw_temp_vals = raw['tdb'].values\n",
    "    \n",
    "    if np.unique(raw.index.year)==2223:\n",
    "        p2 = ax[1].plot(plot_x, raw_temp_vals, zorder = 12, color=colours.grey)\n",
    "    else:\n",
    "        p3 = ax[1].plot(plot_x, raw_temp_vals, zorder = 1, color=colours.orange, alpha=0.25)\n",
    "    \n",
    "    if ymin[1] in raw_temp_vals:\n",
    "        xmin.append(np.argmin(raw_temp_vals))\n",
    "    \n",
    "    if ymax[1] in raw_temp_vals:\n",
    "        xmax.append(np.argmax(raw_temp_vals))\n",
    "\n",
    "#     ax[1].plot(plot_x, np.repeat(summary['tdb'], 8760/12), color=colours.blackest, zorder=11)\n",
    "\n",
    "for ax_temp in ax:\n",
    "    ax_temp.set_ylabel('Dry bulb temperature [$^o$C]')\n",
    "    ax_temp.set_xlim(plot_x[0], plot_x[-1])\n",
    "\n",
    "ax[0].legend(['Smoothed', 'Hourly', 'Monthly'])\n",
    "ax[1].legend({p1[0], p3[0], p2[0]}, {'Smoothed', 'Hourly', 'Monthly'})\n",
    "ax[1].set_xlabel('Hour of the year')\n",
    "ax[0].set_title('Hourly and smoothed values of temperatures for Dehradun')\n",
    "ax[1].set_title('Hourly and smoothed values of temperatures for Geneva')\n",
    "\n",
    "ax[0].annotate('Hottest temperature \\n in the TMY file\\n',\n",
    "    xy=(xmax[0], ymax[0]), arrowprops=dict(arrowstyle='->'), xytext=(4000, 10))\n",
    "ax[0].annotate('Coldest temperature \\n in the TMY file\\n',\n",
    "    xy=(xmin[0], ymin[0]), arrowprops=dict(arrowstyle='->'), xytext=(4000, 5))\n",
    "\n",
    "ax[1].annotate('Hottest temperature \\n in 36 years\\n',\n",
    "    xy=(xmax[1], ymax[1]), arrowprops=dict(arrowstyle='->'), xytext=(5000, -10))\n",
    "ax[1].annotate('Coldest temperature \\n in 36 years\\n',\n",
    "    xy=(xmin[1], ymin[1]), arrowprops=dict(arrowstyle='->'), xytext=(2000, -20))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Black Box Models </h2>\n",
    "\n",
    "A black-box model is any model (mathematical relationship) that can only be viewed in terms of its inputs and outputs. That is, we cannot \"see\" the inner workings or relationships that result in certain inputs giving certain outputs. It can be used to represent a physical system if sufficient data is available to characterise the relationship between the inputs and outputs of that system. In building simulation, for example, a black-box model could relate outside temperature to heating demand without any indication of <i>how</i> that demand is generated physically, i.e., without solving the equations of heat transfer.  \n",
    "\n",
    "For this workshop, we will use <kbd>scikit-learn</kbd> (use the alias <kbd>sklearn</kbd> when installing with <kbd>pip</kbd> or <kbd>conda</kbd>), a library in python, to fit models to data obtained from energy plus simulations.  \n",
    "\n",
    "<h2>Exercises</h2>\n",
    "\n",
    "We will do two exercises, each linked to a different type of uncertainty: \n",
    "\n",
    "<ol>\n",
    "<li> Epistemic uncertainty: fixed inputs, random coefficients. \n",
    "<li> Aleatory uncertainty: random inputs, fixed coefficients.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from energyplus simulations.\n",
    "\n",
    "path_data = '../sim_data/SummaryTable_BaseSimulation.csv'\n",
    "\n",
    "df_eplus = pd.read_csv(path_data)\n",
    "\n",
    "# Drop rows that contain infinity, negative infinity, NaN values.\n",
    "df_eplus = df_eplus.dropna(how='any', axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2> \n",
    "<br>\n",
    "This dataframe contains some numerical fields, which we will use in this regression exercise, and non-numerical fields, which are just labels. \n",
    "\n",
    "The 'outputs' are annual sum of heating and cooling loads - the dataframe contains both the raw <i>kWh</i> values and normalised values in <i>kWh/m2</i>. The inputs are all the remaining numerical fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the data.\n",
    "\n",
    "print('Columns:', df_eplus.columns.tolist(), '\\r\\n==========================')\n",
    "\n",
    "print('Dimensions:', df_eplus.shape, '\\r\\n==========================')\n",
    "\n",
    "print(df_eplus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regression Models</h2>\n",
    "\n",
    "We will define a function that takes as input the x-data (independent variables), the model type (e.g., linear), and the coefficients (optional, if known). We will write the function such that if you do not enter values for the regression coefficients, the function will attempt to fit the x-data to the given y-data.\n",
    "\n",
    "First we split the data into two subsets, one each for <b>training</b> and <b>testing</b>. As the names suggest, we will use the first subset to 'train' the regression models, and then test the models on the second subset. A regression model needs to be trained by showing it data that is properly representative of the problem being studied. This is because the regression model does not solve the underlying physical equations; instead, it tries to mimic the behaviour of the system (building) by modelling the changes in inputs and outputs seen in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are columns with numerical values.\n",
    "xvars = ['cdd', 'hdd', 'avtdp', 'sumghi', 'sumdni',\n",
    "         'sumihg', 'suminfloss', 'suminfgain', 'sumwinloss', 'sumwingain',\n",
    "         'wfr', 'wwr', 'tmass', 'uval', 'rr', 'ff']\n",
    "\n",
    "# Select 50,000 random samples as training data.\n",
    "train_sample = np.random.choice(df_eplus.index, 50000, replace=False)\n",
    "# The rest become test samples.\n",
    "test_sample = [x for x in df_eplus.index if x not in train_sample]\n",
    "\n",
    "# Based on the sampling above, divide the dataset into subsets for training and testing.\n",
    "x_train = df_eplus.loc[train_sample, xvars]\n",
    "y_train = df_eplus.loc[train_sample, ['heatnorm', 'coolnorm']]\n",
    "x_test = df_eplus.loc[test_sample, xvars]\n",
    "y_test = df_eplus.loc[test_sample, ['heatnorm', 'coolnorm']]\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler_x.fit_transform(x_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "x_test_scaled = scaler_x.fit_transform(x_test)\n",
    "y_test_scaled = scaler_y.fit_transform(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Scaling should transform the columns so they have mean=0 and stdev=1.\n",
    "print('Scaled column means:', np.mean(x_train_scaled,axis=0))\n",
    "print('Scaled column stdevs:', np.std(x_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default font properties.\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6)) \n",
    "ax = axes.flatten()\n",
    "fig.tight_layout(pad=3, w_pad=3, h_pad=5)\n",
    "\n",
    "ax[0].hist(y_train['heatnorm'], density=True, color=colours.hotpink, alpha=0.5,\n",
    "           align='left')\n",
    "ax[0].hist(y_test['heatnorm'], density=True, color=colours.hotpink, alpha=1,\n",
    "           histtype='step', align='left', linewidth=3)\n",
    "ax[0].set_title('Distributions - Heating')\n",
    "ax[0].set_ylabel('PDF')\n",
    "ax[0].set_xlabel('y [kWh/$m^2$]')\n",
    "\n",
    "ax[1].hist(y_train['coolnorm'], density=True, color=colours.blue, alpha=0.5,\n",
    "           align='left')\n",
    "ax[1].hist(y_test['coolnorm'], density=True, color=colours.blue, alpha=1,\n",
    "           histtype='step', align='left', linewidth=3)\n",
    "ax[1].set_title('Distributions - Cooling')\n",
    "ax[1].set_ylabel('PDF')\n",
    "ax[1].set_xlabel('y [kWh/$m^2$]')\n",
    "\n",
    "ax[0].grid(True)\n",
    "ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define min and max to set pretty axis limits.\n",
    "axmin = dict(heat=df_eplus.loc[train_sample, 'heatnorm'].min(),\n",
    "             cool=df_eplus.loc[train_sample, 'coolnorm'].min())\n",
    "axmax = dict(heat=df_eplus.loc[train_sample, 'heatnorm'].max().round(-2),\n",
    "             cool=df_eplus.loc[train_sample, 'coolnorm'].max().round(-2))\n",
    "bins = dict(heat=np.linspace(-100, 100, num=25),\n",
    "            cool=np.linspace(-100, 100, num=25))\n",
    "\n",
    "print(axmin)\n",
    "print(axmax)\n",
    "# print(bins)\n",
    "\n",
    "def plot_predictions(y_test, y_pred,y_train):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12)) \n",
    "    ax = axes.flatten()\n",
    "    fig.tight_layout(pad=3, w_pad=3, h_pad=5)\n",
    "\n",
    "    ax[0].plot(y_test['heatnorm'], y_pred['heatnorm'], color=colours.hotpink, marker='.', linewidth=0)\n",
    "    ax[0].plot(y_test['heatnorm'], np.repeat(y_train['heatnorm'].mean(), y_test.shape[0]),\n",
    "               color=colours.blackest, linewidth=2)\n",
    "    ax[0].set_xlim(axmin['heat'], axmax['heat'])\n",
    "    ax[0].set_ylim(axmin['heat'], axmax['heat'])\n",
    "    ax[0].set_title('Heating')\n",
    "    ax[0].set_xlabel('Simulated (E+) [kWh/$m^2$]')\n",
    "    ax[0].set_ylabel('Predicted (regression) [kWh/$m^2$]')\n",
    "    ax[0].legend({'Mean', 'Model'})\n",
    "\n",
    "    ax[1].plot(y_test['coolnorm'], y_pred['coolnorm'], color=colours.blue, marker='.', linewidth=0)\n",
    "    ax[1].plot(y_test['coolnorm'], np.repeat(y_train['coolnorm'].mean(), y_test.shape[0]),\n",
    "               color=colours.blackest, linewidth=2)\n",
    "    ax[1].set_xlim(axmin['cool'], axmax['cool'])\n",
    "    ax[1].set_ylim(axmin['cool'], axmax['cool'])\n",
    "    ax[1].set_title('Cooling')\n",
    "    ax[1].set_xlabel('Simulated (E+) [kWh/$m^2$]')\n",
    "    ax[1].set_ylabel('Predicted (regression) [kWh/$m^2$]')\n",
    "    ax[1].legend({'Mean', 'Model'})\n",
    "    \n",
    "    ax[2].hist(y_test['heatnorm'] - y_pred['heatnorm'], bins=bins['heat'],\n",
    "               density=False, color=colours.hotpink, alpha=0.5, align='left', rwidth=0.75)\n",
    "    ax[2].set_title('Errors - Heating')\n",
    "    ax[2].set_ylabel('Count')\n",
    "    ax[2].set_xlabel('$\\epsilon$ [kWh/$m^2$]')\n",
    "\n",
    "    ax[3].hist(y_test['coolnorm'] - y_pred['coolnorm'], bins=bins['cool'],\n",
    "               density=False, color=colours.blue, alpha=0.5, align='left', rwidth=0.75)\n",
    "    ax[3].set_title('Errors - Cooling')\n",
    "    ax[3].set_ylabel('Count')\n",
    "    ax[3].set_xlabel('$\\epsilon$ [kWh/$m^2$]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single-parameter model</h2>\n",
    "\n",
    "A model with only one parameter represents a situation where you have no idea what to expect, so it is safest to assume that the mean energy usage seen in your training data is representative of the entire population. The standard method of assessing the performance of any model is to compare it to this single-parameter model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dict()\n",
    "y_pred['heatnorm'] = np.repeat(np.mean(y_train.loc[:, 'heatnorm']), y_test.loc[:, 'heatnorm'].shape[0])\n",
    "y_pred['coolnorm'] = np.repeat(np.mean(y_train.loc[:, 'coolnorm']), y_test.loc[:, 'coolnorm'].shape[0])\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear Models</h2>\n",
    "\n",
    "For the first model, we use a linear model to specify the relationship between inputs and output. The simulators you use will almost never be linear - for buildings at least - so this sort of model will usually only work for very simple problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin_regr = dict(heatnorm=LinearRegression(), coolnorm=LinearRegression())\n",
    "model_lin_regr['heatnorm'].fit(X=x_train_scaled, y=y_train['heatnorm'])\n",
    "model_lin_regr['coolnorm'].fit(X=x_train_scaled, y=y_train['coolnorm'])\n",
    "\n",
    "y_pred = np.NaN*np.ones([y_test.shape[0], 2]) \n",
    "\n",
    "y_pred[:, 0] = model_lin_regr['heatnorm'].predict(x_test_scaled)\n",
    "y_pred[:, 1] = model_lin_regr['coolnorm'].predict(x_test_scaled)\n",
    "\n",
    "# Invert the transformation and put the resulting predictions into a dataframe for plotting.\n",
    "y_pred = pd.DataFrame(y_pred, columns=['heatnorm', 'coolnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xvars)\n",
    "\n",
    "print('HEATING')\n",
    "print('Coefficients: ', model_lin_regr['heatnorm'].coef_)\n",
    "print('Intercept: ', model_lin_regr['heatnorm'].intercept_)\n",
    "\n",
    "print('COOLING')\n",
    "print('Coefficients: ', model_lin_regr['coolnorm'].coef_)\n",
    "print('Intercept: ', model_lin_regr['coolnorm'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly2_features = PolynomialFeatures(degree=2)\n",
    "x_train_trans = model_poly2_features.fit_transform(x_train)\n",
    "model_poly2_regr = dict(heatnorm=LinearRegression(), coolnorm=LinearRegression())\n",
    "model_poly2_regr['heatnorm'].fit(X=x_train_trans, y=y_train.loc[:, 'heatnorm'])\n",
    "model_poly2_regr['coolnorm'].fit(X=x_train_trans, y=y_train.loc[:, 'coolnorm'])\n",
    "\n",
    "x_test_trans = model_poly2_features.fit_transform(x_test)\n",
    "\n",
    "y_pred = np.NaN*np.ones([y_test.shape[0], 2]) \n",
    "\n",
    "y_pred[:, 0] = model_poly2_regr['heatnorm'].predict(x_test_trans)\n",
    "y_pred[:, 1] = model_poly2_regr['coolnorm'].predict(x_test_trans)\n",
    "\n",
    "# Invert the transformation and put the resulting predictions into a dataframe for plotting.\n",
    "y_pred = pd.DataFrame(y_pred, columns=['heatnorm', 'coolnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('HEATING')\n",
    "# print('Coefficients: ', model_poly2_regr['heatnorm'].coef_)\n",
    "print('Intercept: ', model_poly2_regr['heatnorm'].intercept_)\n",
    "\n",
    "print('COOLING')\n",
    "# print('Coefficients: ', model_poly2_regr['coolnorm'].coef_)\n",
    "print('Intercept: ', model_poly2_regr['coolnorm'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly3_features = PolynomialFeatures(degree=3)\n",
    "x_train_trans = model_poly3_features.fit_transform(x_train)\n",
    "\n",
    "model_poly3_regr = dict(heatnorm=LinearRegression(), coolnorm=LinearRegression())\n",
    "model_poly3_regr['heatnorm'].fit(X=x_train_trans, y=y_train.iloc[:,0])\n",
    "model_poly3_regr['coolnorm'].fit(X=x_train_trans, y=y_train.iloc[:,1])\n",
    "\n",
    "x_test_trans = model_poly3_features.fit_transform(x_test)\n",
    "\n",
    "y_pred = np.NaN*np.ones([y_test.shape[0], 2]) \n",
    "\n",
    "y_pred[:, 0] = model_poly3_regr['heatnorm'].predict(x_test_trans)\n",
    "y_pred[:, 1] = model_poly3_regr['coolnorm'].predict(x_test_trans)\n",
    "\n",
    "# Invert the transformation and put the resulting predictions into a dataframe for plotting.\n",
    "y_pred = pd.DataFrame(y_pred, columns=['heatnorm', 'coolnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Feature Names: ', model_poly3_features.get_feature_names(input_features=xvars))\n",
    "\n",
    "print('HEATING')\n",
    "# print('Coefficients: ', model_poly3_regr['heatnorm'].coef_)\n",
    "print('Intercept: ', model_poly3_regr['heatnorm'].intercept_)\n",
    "\n",
    "print('COOLING')\n",
    "# print('Coefficients: ', model_poly3_regr['coolnorm'].coef_)\n",
    "print('Intercept: ', model_poly3_regr['coolnorm'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Uncertainty quantification - demonstration </h2>\n",
    "\n",
    "Now that we have demonstrated regression models, let's use them to explore the quantification of uncertainty. We will pretend that the data we have at hand represents a real design problem. \n",
    "\n",
    "Remember that, in our exercises, the coefficients of the regression equation represent the magnitude of effect each input or property has on the output. The regression inputs are varying values of building properties (e.g., internal heat gain) and boundary conditions (e.g., weather). Once the building's properties are fixed, variation in the boundary conditions creates variation in energy and other outputs of interest (like indoor temperature). During design, however, it makes sense to test each building property over many boundary conditions to ensure that performance is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epistemic Uncertainty </h2>\n",
    "\n",
    "<h3>Random coefficients, fixed inputs</h3>\n",
    "\n",
    "For the first exercise, we fix the inputs to typical values and use random coefficients to specify the relationship between inputs and output. The simulators you use will almost never be random - for buildings at least - so this exercise represents a lack of knowledge about the coefficients. That is to say, <i>if we don't know the relationships, then they appear random</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a function with 5 variables (to keep this exercise simple, we will use only five input variables).\n",
    "\n",
    "def model_random_regr(load, x, model_type):\n",
    "    \n",
    "    beta = dict(heatnorm=None, coolnorm=None)\n",
    "    \n",
    "    if model_type == 'random':\n",
    "        beta['heatnorm'] = np.random.rand(x.size)\n",
    "        beta['coolnorm'] = np.random.rand(x.size)\n",
    "        \n",
    "    elif model_type == 'fixed':\n",
    "        beta['heatnorm'] = [1, 1, 1, 1, 1]\n",
    "        beta['coolnorm'] = [1, 1, 1, 1, 1]\n",
    "\n",
    "    if load in ['cool', 'coolnorm']:\n",
    "        y = np.sum([b*xin for b, xin in zip(beta['coolnorm'], x)])\n",
    "    elif load in ['heat', 'heatnorm']:\n",
    "        y = np.sum([b*xin for b,xin in zip(beta['heatnorm'], x)])\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the function pick any random coefficients. \n",
    "# The output will look different every time.\n",
    "\n",
    "y_pred = np.NaN*np.ones([y_test.shape[0], 2]) \n",
    "\n",
    "for idx, x in enumerate(x_test_scaled):\n",
    "    y_pred[idx, 0] = (model_random_regr('heatnorm', x, 'random'))\n",
    "    y_pred[idx, 1] = (model_random_regr('coolnorm', x, 'random'))\n",
    "\n",
    "# Inverse the transformation and put the resulting predictions into a dataframe for plotting.\n",
    "y_pred = pd.DataFrame(scaler_y.inverse_transform(y_pred), columns=['heatnorm', 'coolnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we thought that each input had exactly the same impact on the output?\n",
    "\n",
    "y_pred = np.NaN*np.ones([y_test.shape[0], 2]) \n",
    "\n",
    "for idx, x in enumerate(x_test_scaled):\n",
    "    y_pred[idx, 0] = (model_random_regr('heatnorm', x, 'fixed'))\n",
    "    y_pred[idx, 1] = (model_random_regr('coolnorm', x, 'fixed'))\n",
    "\n",
    "y_pred = pd.DataFrame(scaler_y.inverse_transform(y_pred), columns=['heatnorm', 'coolnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, y_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Aleatory Uncertainty </h2>\n",
    "\n",
    "<h3>Fixed coefficients, random inputs</h3>\n",
    "\n",
    "For this exercise, we will learn the coefficients on typical inputs and use these to examine the response of the building to random inputs (varying outdoor or boundary conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical weather only.\n",
    "x_train_typ = df_eplus.loc[df_eplus.loc[:, 'wthrsrc']=='typical', xvars]\n",
    "y_train_typ = df_eplus.loc[df_eplus.loc[:, 'wthrsrc']=='typical', ['heatnorm', 'coolnorm']]\n",
    "\n",
    "# Measured and synthetic weather.\n",
    "x_test_typ = df_eplus.loc[df_eplus.loc[:, 'wthrsrc']!='typical', xvars]\n",
    "y_test_typ = df_eplus.loc[df_eplus.loc[:, 'wthrsrc']!='typical', ['heatnorm', 'coolnorm']]\n",
    "\n",
    "y_pred_typ = dict(heatnorm=None, coolnorm=None)\n",
    "\n",
    "# Let's fit a second-degree polynomial.\n",
    "model_poly_typ_features = PolynomialFeatures(2)\n",
    "x_train_trans = model_poly_typ_features.fit_transform(x_train_typ)\n",
    "model_poly_typ_regr = dict(heatnorm=LinearRegression(), coolnorm=LinearRegression())\n",
    "model_poly_typ_regr['heatnorm'].fit(X=x_train_trans, y=y_train_typ['heatnorm'])\n",
    "model_poly_typ_regr['coolnorm'].fit(X=x_train_trans, y=y_train_typ['coolnorm'])\n",
    "\n",
    "x_test_trans = model_poly_typ_features.fit_transform(x_test_typ)\n",
    "y_pred_typ['heatnorm'] = model_poly_typ_regr['heatnorm'].predict(x_test_trans)\n",
    "y_pred_typ['coolnorm'] = model_poly_typ_regr['coolnorm'].predict(x_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test_typ, y_pred_typ, y_train_typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6)) \n",
    "ax = axes.flatten()\n",
    "fig.tight_layout(pad=3, w_pad=3, h_pad=5)\n",
    "\n",
    "ax[0].hist(y_train_typ['heatnorm'], density=True, color=colours.blue, alpha=0.5,\n",
    "           align='left')\n",
    "ax[0].hist(y_test_typ['heatnorm'], density=True, color=colours.blue, alpha=1,\n",
    "           histtype='step', align='left', linewidth=3)\n",
    "ax[0].set_title('Distributions - Heating')\n",
    "ax[0].set_ylabel('PDF')\n",
    "ax[0].set_xlabel('y [kWh/$m^2$]')\n",
    "\n",
    "ax[1].hist(y_train_typ['coolnorm'], density=True, color=colours.hotpink, alpha=0.5,\n",
    "           align='left')\n",
    "ax[1].hist(y_test_typ['coolnorm'], density=True, color=colours.hotpink, alpha=1,\n",
    "           histtype='step', align='left', linewidth=3)\n",
    "ax[1].set_title('Distributions - Cooling')\n",
    "ax[1].set_ylabel('PDF')\n",
    "ax[1].set_xlabel('y [kWh/$m^2$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Individual projects </h2>\n",
    "\n",
    "See handout.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Optional exercise: solar power with random weather inputs</h2>\n",
    "\n",
    "<strong>Optional</strong>\n",
    "\n",
    "Use the `solar_power_func` to plot solar power production with random inputs. This function uses the procedure laid out in the iPython notebook: `solar_power`. \n",
    "\n",
    "Bonus exercise: optimise tilt and/or orientation angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am re-issuing the import commands here so this part stands alone.\n",
    "# The modules that are already loaded will not reload. \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import solar_power_func as solar\n",
    "import wfileio as wf\n",
    "import default_colours as colours\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = 'epw'\n",
    "\n",
    "# I use os.path.join so the paths are automatically constructed based on the OS.\n",
    "path_epw_ddn = '../India_Dehradun/IND_UT_Dehradun.421110_ISHRAE2014.epw'\n",
    "\n",
    "# The small program get_weather stores data from the incoming weather\n",
    "# file as a dataframe.\n",
    "ddn, locdata, header = wf.get_weather('ddn', path_epw_ddn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(solar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = solar.tmy_to_power(path_tmy_data=path_epw_ddn,\n",
    "                           surface_tilt=30, surface_azimuth=180,\n",
    "                           albedo=0.2, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.arange(0, ddn.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(plot_x, power, zorder = 1, color=colours.orange, alpha=0.5)\n",
    "plt.ylabel('AC power [W]')\n",
    "plt.xlabel('Hour of the year')\n",
    "plt.xlim(plot_x[0], plot_x[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with Geneva data. Except Geneva has multiple weather files, so use a loop.\n",
    "path_gen_folder = os.path.join('..', 'Switzerland_Geneva', 'historical')\n",
    "list_genfiles = glob.glob(path_gen_folder + '/*.{:s}'.format(file_type))\n",
    "\n",
    "# Declare a list.\n",
    "gen_list = list()\n",
    "\n",
    "for file in list_genfiles:\n",
    "    \n",
    "    gen_temp, locdata, header = wf.get_weather('gen', file)\n",
    "    gen_list.append(gen_temp)\n",
    "    \n",
    "# Keep the list - useful for plotting and stuff later.\n",
    "gen = pd.concat(gen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_syn_list = list()\n",
    "\n",
    "for file in list_genfiles:\n",
    "    power_syn_list.append(solar.tmy_to_power(path_tmy_data=file,\n",
    "                                   surface_tilt=30, surface_azimuth=180,\n",
    "                                   albedo=0.2, silent=True))\n",
    "\n",
    "power_syn_df = pd.concat(power_syn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_syn_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.arange(0, power_syn_df.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(plot_x, power_syn_df, zorder = 1, color=colours.orange, alpha=0.5)\n",
    "plt.ylabel('AC power [W]')\n",
    "plt.xlabel('Hour of the year')\n",
    "plt.xlim(plot_x[0], plot_x[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Bibliography </h2>\n",
    "\n",
    "<ul>\n",
    "<li>Rastogi, Parag. 2016. “On the Sensitivity of Buildings to Climate: The Interaction of Weather and Building Envelopes in Determining Future Building Energy Consumption.” PhD, Lausanne, Switzerland: Ecole polytechnique fédérale de Lausanne. EPFL Infoscience. https://infoscience.epfl.ch/record/220971?ln=en.\n",
    "<li>Rastogi, Parag, and Marilyne Andersen. 2015. “Embedding Stochasticity in Building Simulation Through Synthetic Weather Files.” In Proceedings of BS 2015. Hyderabad, India. http://infoscience.epfl.ch/record/208743.\n",
    "<li>———. 2016. “Incorporating Climate Change Predictions in the Analysis of Weather-Based Uncertainty.” In Proceedings of SimBuild 2016. Salt Lake City, UT, USA. http://infoscience.epfl.ch/record/208743.\n",
    "<li>Rastogi, Parag, Mohammad Emtiyaz Khan, and Marilyne Andersen. 2017. “Gaussian-Process-Based Emulators for Building Performance Simulation.” In Proceedings of BS 2017. San Francisco, CA, USA: IBPSA.\n",
    "<li>Iaccarino, Gianluca. 2008. “Quantification of Uncertainty in Flow Simulations Using Probabilistic Methods.” presented at the VKI Lecture Series, Stanford University, September. http://web.stanford.edu/group/uq/uq_youq.html.\n",
    "<li>Macdonald, Iain. 2002. “Quantifying the Effects of Uncertainty in Building Simulation.” Doctoral, University of Strathclyde. https://www.strath.ac.uk/media/departments/mechanicalengineering/esru/research/phdmphilprojects/macdonald_thesis.pdf.\n",
    "\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
